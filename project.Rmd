---
title: "Project"
author: "Manuel Alejandro"
date: "5/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
 
```

```{r}
library(readr)
library(lubridate)
library(ggplot2)
library(Metrics)
library(dplyr)
library(zoo)
library(e1071)
```
#### Installation von Python und der für TensorFlow benötigten Pakete (nur einmalig nötig) ###
```{r}
library(reticulate)

# Installation von miniconda (falls nicht vorhanden)
install_miniconda()

# Anlegen einer speziellen Python Umgebung
conda_create("r-reticulate")

# Installieren der Pakete in der angelegten Umgebung
conda_install("r-reticulate", "pandas")
conda_install("r-reticulate", "numpy")
conda_install("r-reticulate", "tensorflow")

# Verwenden der speziellen Python Umgebung die zuvor erstellt wurde
use_condaenv("r-reticulate")

```

```{r}
umsatzdaten <- read_csv("umsatzdaten_gekuerzt.csv")
```
Erstellung der Variable mit Wochentag
```{r}
# Berechnung der Wochentage
umsatzdaten$wochentag <- weekdays(umsatzdaten$Datum)

# Umwandlung von 'wochentag" in eine Faktor-Variable mit einer vorgegeben Sortierung der Level (Kategorien)
umsatzdaten$wochentag <- factor(umsatzdaten$wochentag, levels=c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

```

Alle Warengruppen eines Tages zusammen fügen
```{r}

```

Erstellung der Variable mit Jahreszeit
```{r}
# Berechnung der Jahresqrt
yq <- as.yearqtr(as.yearmon(umsatzdaten$Datum, "%m/%d/%Y") + 1/12)
# Berechnung der Jahreszeit
umsatzdaten$jahreszeit <- factor(format(yq, "%q"), levels = 1:4, 
      labels = c("Winter", "Frueling", "Sommer", "Herbst"))


```

Erstellung der Variable mit Feiertage 
```{r}
#Einbinden der Bibliothek mit dr Funktion isHoliday:
detach("package:timeDate", unload = TRUE)
library(RQuantLib)

#Zeige deutsche Feiertage an:
getHolidayList("Germany", as.Date("2013-01-01"), as.Date("2020-01-01"))

#Füge die Feiertage "Himmelfahrt", "Pfingsten", "Tag deutscher Einheit" und "Reformationstag" zwischen 2013 und 2022 hinzu:
further_holidays <- (as.Date(c("2013-10-03", "2014-10-03", "2015-10-03", "2016-10-03", "2017-10-03", "2018-10-03", "2019-10-03", "2020-10-03", "2021-10-03", "2017-10-31", "2018-10-31", "2019-10-31", "2020-10-31", "2021-10-31", "2022-10-31", "2013-05-09", "2014-05-29", "2015-05-14", "2016-05-05", "2017-05-25", "2018-05-10", "2019-05-30", "2020-05-21", "2021-05-13", "2022-05-26", "2013-05-20", "2014-06-09", "2015-05-25", "2016-05-16", "2017-06-05", "2018-05-21", "2019-06-10", "2020-06-01", "2021-05-24", "2022-06-06", "2013-12-26", "2014-12-26", "2015-12-26", "2016-12-26", "2017-12-26", "2018-12-26", "2019-12-26", "2020-12-26", "2021-12-26", "2022-12-26")))
addHolidays("Germany", further_holidays)

#Erstelle neue Spalte mit dem Wert für Feiertag (Wochenenden ausgenommen):
umsatzdaten$holiday <- isHoliday("Germany", umsatzdaten$Datum) & !isWeekend("Germany", umsatzdaten$Datum)

View(umsatzdaten)
```

Langes Wochendende Vorher und nachher
Feiertag am Wochenende
```{r}




```

hat der Bäcker an manchen Tagen geschlossen = kein Umsatz
Vorher/nachher mehr umsatz 
```{r}
detach("package:RQuantLib", unload = TRUE)
library(timeDate)
all_dates <- tibble(Datum = as.Date(timeSequence(as.Date("2013-07-01"),as.Date("2019-06-03"))))

missing_dates <- anti_join(all_dates, umsatzdaten)


detach("package:timeDate", unload = TRUE)
library(RQuantLib)
missing_dates$holiday <- isHoliday("Germany", missing_dates$Datum)
missing_dates$WochentagMoFr <- !isWeekend("Germany", missing_dates$Datum)

```

Schulferien
```{r}
AshWednesday() #Funktion für Ferien in Deutschland
#Ferien
```


Kieler Woche:
1. Einbinden benötigter Bibliotheken
2. Einlesen der Daten
3. Zusammenführen der Daten
```{r}
library(readr)
library(dplyr)

#umsatzdaten <- read_csv("umsatzdaten_gekuerzt.csv")
kiwo <- read_csv("kiwo.csv")


umsatzdaten <- left_join(umsatzdaten, kiwo)
View(umsatzdaten)
```


Einbindung Wetterdaten
```{r}
library(readr)
library(dplyr)

wetter <- read_csv("wetter.csv")
View(wetter)

umsatzdaten<- left_join(umsatzdaten, wetter)
View(umsatzdaten)



```



```Overfitting with linear regression
ToDo: Noch updaten!!
Importing Function Packages

```{r}
library(dplyr)
library(readr)
library(lubridate)
library(broom)
library(Metrics)
```

Datensatz in Training und Test Data teilen
https://duttashi.github.io/blog/splitting-a-data-frame-into-training-and-testing-sets-in-r/


```{r}
library(caTools)
# To set a seed use the function set.seed()
set.seed(123)

#70% training and 30% testing data
# umsatzdaten$spl will create a new column in the umsatzdaten dataset.</code>
umsatzdaten$spl=sample.split(umsatzdaten$Datum,SplitRatio=0.7)

View(umsatzdaten)
#Training data set
train=subset(umsatzdaten, umsatzdaten$spl==TRUE)
# where <i>spl== TRUE</i> means to add only those rows that have value true for spl in the training dataframe

# you will see that this dataframe has all values where umsatzdaten$spl==TRUE
View(train)
write.csv(train,"train.csv", row.names = TRUE)
#Similarly, to create the testing dataset,
test=subset(umsatzdaten, umsatzdaten$spl==FALSE) 
View(test)
write.csv(test,"test.csv", row.names = TRUE)
#where <i>spl== FALSE </i> means to add only those rows that have value true for spl in the training dataframe

#> View(test) # you will see that this dataframe has all values where iris$spl==FALSE
```

```{r}
# Uncomment the following line to check the correctness of the code with a small (and computationally fast) training data set
train_dataset <- sample_frac(train, .10)
```

Importing Training and Test Data


```{r}
baecker_umsatzdaten_train <- read_csv("train.csv")
baecker_umsatzdaten_test <- read_csv("test.csv")
```

Estimating (Training) Models

(Alle Warengruppen getrennt anschauen)
```{r}
mod1 <- lm(Umsatz ~ wochentag, umsatzdaten)
mod2 <- lm(Umsatz ~ as.factor(Warengruppe), umsatzdaten)
mod3 <- lm(Umsatz ~ as.factor(Warengruppe)+ as.factor(wochentag), umsatzdaten)
mod4 <- lm(Umsatz ~ as.factor(Warengruppe)+ as.factor(wochentag)+ KielerWoche, umsatzdaten)

mod5 <- lm(Umsatz ~ as.factor(Warengruppe)+ as.factor(wochentag)+ KielerWoche+ Temperatur, umsatzdaten)
mod6 <- lm(Umsatz ~ as.factor(Warengruppe)+ as.factor(wochentag)+ KielerWoche+ Temperatur+holiday, umsatzdaten) 
#mod7 <- lm(Umsatz ~ as.factor(Warengruppe)+ as.factor(wochentag)+ KielerWoche+ Temperatur+holiday+as.factor(jahreszeit), umsatzdaten) #Fehler

mod8 <- lm(Umsatz ~ as.factor(Warengruppe)+ as.factor(wochentag)+ KielerWoche+ Temperatur+holiday+Bewoelkung, umsatzdaten) 

```


```{r}
summary(mod1)
plot(mod1)
summary(mod2)
plot(mod2)
summary(mod3)
summary(mod4)
summary(mod5)
summary(mod6)
#summary(mod7)
summary(mod8)
```

```{r}

glance(mod1)
glance(mod2)
```

Preparation of Model Results
```{r}
rbind(glance(mod1), glance(mod2), glance(mod3), glance(mod4), glance(mod5), glance(mod6), glance(mod7))
```

Model Prediction Quality for the Training Data Using the Mean Absolute Error
```{r}
rbind(mae(house_pricing_train$price, predict(mod1)),
      mae(house_pricing_train$price, predict(mod2)),
      mae(house_pricing_train$price, predict(mod3)),
      mae(house_pricing_train$price, predict(mod4)),
      mae(house_pricing_train$price, predict(mod5)),
      mae(house_pricing_train$price, predict(mod6)),
      mae(house_pricing_train$price, predict(mod7)))
```


Model Prediction Quality for the Training Data Using the Mean Absolute Percentage Error
```{r}
rbind(mape(house_pricing_train$price, predict(mod1)),
      mape(house_pricing_train$price, predict(mod2)),
      mape(house_pricing_train$price, predict(mod3)),
      mape(house_pricing_train$price, predict(mod4)),
      mape(house_pricing_train$price, predict(mod5)),
      mape(house_pricing_train$price, predict(mod6)),
      mape(house_pricing_train$price, predict(mod7)))
```

Model Prediction Quality for the (Unknown) Test Data Using the Mean Absolute Percentage Error
```{r}
rbind(mape(house_pricing_test$price, predict(mod1, newdata=house_pricing_test)),
      mape(house_pricing_test$price, predict(mod2, newdata=house_pricing_test)),
      mape(house_pricing_test$price, predict(mod3, newdata=house_pricing_test)),
      mape(house_pricing_test$price, predict(mod4, newdata=house_pricing_test)),
      mape(house_pricing_test$price, predict(mod5, newdata=house_pricing_test)),
      mape(house_pricing_test$price, predict(mod6, newdata=house_pricing_test)),
      mape(house_pricing_test$price, predict(mod7, newdata=house_pricing_test)))


```


## Training the SVM  -- TO DO!
```{r}

# Estimation of an SVM with optimized weighting parameters and given standard hyper parameters
# Typically not used; instead, the function svm_tune is used in order to also get a model with optimized hyper parameters
model_svm <- svm(Umsatz ~ wochentag, train_dataset)
```

```{r}
# Estimation of various SVM (each with optimized weighting parameters) using systematically varied hyper parameters (typically called 'grid search' approach) and cross validation
# the resulting object includes the optimal model in the element named 'best.model'
svm_tune <- tune(svm, Umsatz ~ wochentag, data=train_dataset,
                 ranges = list(epsilon = seq(0.2,1,0.1), cost = 2^(2:3)))
```

## Checking the Prediction Quality -- TO DO!
```{r}
# Calculating the prediction for the training data using the best model according to the grid search
pred_train <- predict(svm_tune$best.model, train_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(train_dataset$price, pred_train)
```


```{r}
# Calculating the prediction for the test data using the best model according to the grid search
pred_test <- predict(svm_tune$best.model, test_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(test_dataset$price, pred_test)
```



#TO DO
### Installation von Python und der für TensorFlow benötigten Pakete (nur einmalig nötig) ###

```{r}
install.packages("reticulate")
library(reticulate)

# Installation von miniconda (falls nicht vorhanden)
install_miniconda()

# Anlegen einer speziellen Python Umgebung
conda_create("r-reticulate")

# Installieren der Pakete in der angelegten Umgebung
conda_install("r-reticulate", "pandas")
conda_install("r-reticulate", "numpy")
conda_install("r-reticulate", "tensorflow")

# Verwenden der speziellen Python Umgebung die zuvor erstellt wurde
use_condaenv("r-reticulate")

```

#TO DO
Estimation of Neural Net Data Preparation
### Vorbereitung der Umgebung ###
```{r}
# Falls nicht installiert ggf. ausausführen
#install.packages("fastDummies")

# Umgebungsvariablen löschen
remove(list = ls())

# Einbinden benötigter Funktionsbibliotheken
library(readr)
library(fastDummies)

```


#TO DO
### Funktionsdefinitionen ###

```{r}
#' Title Fast creation of normalized variables
#' Quickly create normalized columns from numeric type columns in the input data. This function is useful for statistical analysis when you want normalized columns rather than the actual columns.
#'
#' @param .data An object with the data set you want to make normalized columns from.
#' @param norm_values Dataframe of column names, means, and standard deviations that is used to create corresponding normalized variables from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) with same number of rows an dcolumns as the inputted data, only with normalized columns for the variables indicated in the norm_values argument.
#' @export
#'
#' @examples
norm_cols <- function (.data, norm_values = NULL) {
  for (i in 1:nrow(norm_values)  ) {
    .data[[norm_values$name[i]]] <- (.data[[norm_values$name[i]]] - norm_values$mean[i]) / norm_values$sd[i]
  }
  return (.data)
}


#' Title Creation of a Dataframe including the Information to Standardize Variables
#' This function is meant to be used in combination with the function norm_cols
#'
#' @param .data A data set including the variables you want to get the means and standard deviations from.
#' @param select_columns A vector with a list of variable names for which you want to get the means and standard deviations from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) including the names, means, and standard deviations of the variables included in the select_columns argument.
#' @export
#'
#' @examples
get.norm_values <- function (.data, select_columns = NULL) {
  result <- NULL
  for (col_name in select_columns) {
    mean <- mean(.data[[col_name]], na.rm = TRUE)
    sd <- sd(.data[[col_name]], na.rm = TRUE)
    result <- rbind (result, c(mean, sd))
  }
  result <- as.data.frame(result, stringsAsFactors = FALSE)
  result <- data.frame (select_columns, result, stringsAsFactors = FALSE)
  names(result) <- c("name", "mean", "sd")
  return (result)
}

```

#TO DO
### Datenimport ###
```{r}
# Einlesen der Daten
house_pricing <- read_csv("https://raw.githubusercontent.com/opencampus-sh/sose20-datascience/master/house_pricing_test.csv")

```

#TO DO
### Datenaufbereitung ###

```{r}
# Rekodierung von kategoriellen Variablen (zu Dummy-Variablen)
dummy_list <- c("view", "condition")
house_pricing_dummy = dummy_cols(house_pricing, dummy_list)

# Definition von Variablenlisten für die Dummies, um das Arbeiten mit diesen zu erleichtern
condition_dummies = c('condition_1', 'condition_2', 'condition_3', 'condition_4', 'condition_5')
view_dummies = c('view_0', 'view_1', 'view_2', 'view_3','view_4')


# Standardisierung aller Feature Variablen und der Label Variable
norm_list <- c("price", "sqft_lot", "bathrooms", "grade", "waterfront", view_dummies, condition_dummies) # Liste aller Variablen
norm_values_list <- get.norm_values(house_pricing_dummy, norm_list)    # Berechnung der Mittelwerte und Std.-Abw. der Variablen
house_pricing_norm <- norm_cols(house_pricing_dummy, norm_values_list) # Standardisierung der Variablen
```

#TO DO
### Definition der Feaure-Variablen und der Label-Variable

```{r}
# Definition der Features (der unabhängigen Variablen auf deren Basis die Vorhersagen erzeugt werden sollen)
features = c('sqft_lot', 'waterfront', 'grade', 'bathrooms', view_dummies, condition_dummies)
# Definition der Label-Variable (der abhaengigen Variable, die vorhergesagt werden soll) sowie
label = 'price_norm'
```

#TO DO
### Definition von Trainings- und Testdatensatz ###

```{r}
# Zufallszähler setzen, um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten
set.seed(1)
# Bestimmung der Indizes des Traininsdatensatzes
train_ind <- sample(seq_len(nrow(house_pricing_norm)), size = floor(0.80 * nrow(house_pricing_norm)))

# Teilen in Trainings- und Testdatensatz
train_dataset = house_pricing_norm[train_ind, features]
test_dataset = house_pricing_norm[-train_ind, features]

# Selektion der Variable, die als Label definiert wurde
train_labels = house_pricing_norm[train_ind, label]
test_labels = house_pricing_norm[-train_ind, label]


```

#TO DO
```{r}
# Import Libraries
library(reticulate)


# Importing Data
data <- mpg

```


```{python}
year = r.data['year']

```


```{r}
table(py$year)

```




